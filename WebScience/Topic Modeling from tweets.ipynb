{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim import similarities\n",
    "from gensim.utils import  simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1. Use topic modelling on tweet’s text data, then analyse the results and \n",
    "identify issues with short text topic modelling. </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'son', 'and', 'went', 'on', 'tour', 'to', 'the', 'allianz', 'arena', 'we', 'were', 'admiring', 'the', 'seat', 'arena', 'when', 'he', 'suddenly', 'pointed', 'at', 'the', 'pitch', 'dad', 'who', 'are', 'those', 'men', 'camping', 'there', 'said', 'son', 'they', 'are', 'penandes', 'amp', 'penaldo', 'they', 'live', 'in', 'that', 'penalty', 'box', 'they', 'only', 'perform', 'in', 'small', 'games']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  \n",
    "        \n",
    "# Convert to list\n",
    "data = data_df.text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(data_words)  # processed Text Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.089*\"amp\" + 0.047*\"great\" + 0.045*\"people\" + 0.034*\"work\" + 0.030*\"much\" '\n",
      "  '+ 0.020*\"child\" + 0.019*\"tonight\" + 0.018*\"girl\" + 0.017*\"care\" + '\n",
      "  '0.016*\"issue\"'),\n",
      " (1,\n",
      "  '0.182*\"england\" + 0.055*\"game\" + 0.052*\"euro\" + 0.043*\"play\" + 0.040*\"win\" '\n",
      "  '+ 0.037*\"team\" + 0.036*\"tomorrow\" + 0.033*\"germany\" + 0.031*\"football\" + '\n",
      "  '0.021*\"well\"'),\n",
      " (2,\n",
      "  '0.039*\"pay\" + 0.029*\"sure\" + 0.021*\"chance\" + 0.021*\"point\" + 0.018*\"uk\" + '\n",
      "  '0.017*\"shit\" + 0.017*\"low\" + 0.017*\"understand\" + 0.017*\"small\" + '\n",
      "  '0.016*\"car\"'),\n",
      " (3,\n",
      "  '0.044*\"miss\" + 0.041*\"next\" + 0.034*\"penalty\" + 0.026*\"little\" + '\n",
      "  '0.022*\"offer\" + 0.019*\"send\" + 0.018*\"write\" + 0.018*\"ticket\" + 0.017*\"min\" '\n",
      "  '+ 0.014*\"pick\"'),\n",
      " (4,\n",
      "  '0.036*\"pm\" + 0.022*\"match\" + 0.020*\"happen\" + 0.016*\"start\" + 0.015*\"uk\" + '\n",
      "  '0.015*\"july\" + 0.014*\"club\" + 0.014*\"huge\" + 0.013*\"evening\" + '\n",
      "  '0.013*\"claim\"'),\n",
      " (5,\n",
      "  '0.064*\"watch\" + 0.028*\"bring\" + 0.024*\"guy\" + 0.024*\"give\" + 0.021*\"yet\" + '\n",
      "  '0.020*\"oxford\" + 0.019*\"remember\" + 0.018*\"police\" + 0.018*\"second\" + '\n",
      "  '0.018*\"mate\"'),\n",
      " (6,\n",
      "  '0.035*\"report\" + 0.033*\"wait\" + 0.030*\"hear\" + 0.029*\"woman\" + '\n",
      "  '0.023*\"prime_minister\" + 0.021*\"north\" + 0.019*\"wish\" + 0.017*\"grow\" + '\n",
      "  '0.016*\"pass\" + 0.016*\"eat\"'),\n",
      " (7,\n",
      "  '0.080*\"scotland\" + 0.035*\"feel\" + 0.035*\"fan\" + 0.034*\"goal\" + 0.033*\"mean\" '\n",
      "  '+ 0.023*\"move\" + 0.023*\"hour\" + 0.018*\"fact\" + 0.018*\"high\" + 0.017*\"open\"'),\n",
      " (8,\n",
      "  '0.032*\"break\" + 0.029*\"add\" + 0.023*\"happy\" + 0.022*\"month\" + '\n",
      "  '0.021*\"friend\" + 0.018*\"complete\" + 0.016*\"view\" + 0.016*\"drop\" + '\n",
      "  '0.015*\"currently\" + 0.014*\"design\"'),\n",
      " (9,\n",
      "  '0.039*\"really\" + 0.034*\"hope\" + 0.034*\"way\" + 0.028*\"conservative\" + '\n",
      "  '0.022*\"actually\" + 0.019*\"turn\" + 0.019*\"life\" + 0.018*\"people\" + '\n",
      "  '0.017*\"believe\" + 0.017*\"country\"'),\n",
      " (10,\n",
      "  '0.054*\"france\" + 0.052*\"love\" + 0.038*\"thing\" + 0.035*\"never\" + '\n",
      "  '0.031*\"live\" + 0.025*\"change\" + 0.022*\"ever\" + 0.020*\"world\" + 0.018*\"ask\" '\n",
      "  '+ 0.017*\"save\"'),\n",
      " (11,\n",
      "  '0.086*\"covid\" + 0.055*\"man\" + 0.045*\"case\" + 0.032*\"uk\" + 0.030*\"death\" + '\n",
      "  '0.025*\"travel\" + 0.021*\"test\" + 0.021*\"rule\" + 0.020*\"cause\" + '\n",
      "  '0.014*\"soon\"'),\n",
      " (12,\n",
      "  '0.205*\"new\" + 0.115*\"york\" + 0.041*\"city\" + 0.030*\"keep\" + 0.027*\"far\" + '\n",
      "  '0.025*\"time\" + 0.023*\"back\" + 0.018*\"link\" + 0.015*\"early\" + '\n",
      "  '0.014*\"problem\"'),\n",
      " (13,\n",
      "  '0.038*\"sign\" + 0.028*\"set\" + 0.025*\"lead\" + 0.021*\"swiss\" + 0.019*\"base\" + '\n",
      "  '0.018*\"road\" + 0.016*\"build\" + 0.016*\"step\" + 0.016*\"role\" + 0.015*\"st\"'),\n",
      " (14,\n",
      "  '0.071*\"day\" + 0.047*\"london\" + 0.034*\"end\" + 0.032*\"bad\" + 0.030*\"school\" + '\n",
      "  '0.029*\"today\" + 0.026*\"vaccine\" + 0.023*\"still\" + 0.023*\"first\" + '\n",
      "  '0.021*\"week\"'),\n",
      " (15,\n",
      "  '0.317*\"https\" + 0.227*\"co\" + 0.017*\"amp\" + 0.016*\"london\" + 0.007*\"check\" + '\n",
      "  '0.007*\"find\" + 0.006*\"free\" + 0.006*\"help\" + 0.006*\"visit\" + 0.005*\"live\"'),\n",
      " (16,\n",
      "  '0.092*\"loveisland\" + 0.037*\"tell\" + 0.024*\"already\" + 0.022*\"house\" + '\n",
      "  '0.020*\"ill\" + 0.020*\"fucking\" + 0.019*\"kid\" + 0.019*\"price\" + 0.017*\"sound\" '\n",
      "  '+ 0.017*\"street\"'),\n",
      " (17,\n",
      "  '0.059*\"labour\" + 0.026*\"tory\" + 0.025*\"year\" + 0.025*\"government\" + '\n",
      "  '0.025*\"vote\" + 0.022*\"party\" + 0.019*\"people\" + 0.018*\"boris\" + '\n",
      "  '0.018*\"name\" + 0.017*\"player\"'),\n",
      " (18,\n",
      "  '0.043*\"race\" + 0.029*\"least\" + 0.028*\"gon\" + 0.027*\"true\" + 0.024*\"agree\" + '\n",
      "  '0.023*\"able\" + 0.021*\"usa\" + 0.016*\"attack\" + 0.015*\"cost\" + 0.014*\"proud\"'),\n",
      " (19,\n",
      "  '0.048*\"leave\" + 0.039*\"talk\" + 0.034*\"always\" + 0.023*\"lose\" + 0.023*\"long\" '\n",
      "  '+ 0.019*\"uk\" + 0.019*\"away\" + 0.018*\"part\" + 0.018*\"manchester\" + '\n",
      "  '0.017*\"british\"')]\n"
     ]
    }
   ],
   "source": [
    "# Build LDA model 20 topics chunksize 1000\n",
    "lda_20_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=1000,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "pprint(lda_20_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save/load model\n",
    "#lda_20_model.save('lda_20.model')\n",
    "#lda_20_model = gensim.models.ldamodel.LdaModel.load('lda_20.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model 50 topics chunksize 1000\n",
    "lda_50_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=50, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=1000,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda_50_model.save('lda_50.model')\n",
    "#lda_50_model = gensim.models.ldamodel.LdaModel.load('lda_50.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "# aggregate lda 20 topics model\n",
    "lda_20_df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_20_model, corpus=corpus, texts=data_ready)\n",
    "lda_20_df_dominant_topic = lda_20_df_topic_sents_keywords.reset_index()\n",
    "lda_20_df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate lda 50 topics model\n",
    "lda_50_df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_50_model, corpus=corpus, texts=data_ready)\n",
    "lda_50_df_dominant_topic = lda_50_df_topic_sents_keywords.reset_index()\n",
    "lda_50_df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Statistics of Original Tweets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53399.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>42.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Original Tweets\n",
       "count         53399.00\n",
       "mean             11.54\n",
       "std               6.61\n",
       "min               0.00\n",
       "25%               6.00\n",
       "50%              10.00\n",
       "75%              16.00\n",
       "max              42.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_lens = [len(d) for d in lda_20_df_dominant_topic.Text]\n",
    "tweets_df = pd.DataFrame(data=doc_lens)\n",
    "original = round(tweets_df.describe(),2)\n",
    "original.columns = ['Original Tweets']\n",
    "\n",
    "display(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Representative Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8100</td>\n",
       "      <td>amp, great, people, work, much, child, tonight, girl, care, issue</td>\n",
       "      <td>[fabulous, opportunity, great, place]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8643</td>\n",
       "      <td>england, game, euro, play, win, team, tomorrow, germany, football, well</td>\n",
       "      <td>[england, beat, germany, tomorrow, england, win]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8227</td>\n",
       "      <td>pay, sure, chance, point, uk, shit, low, understand, small, car</td>\n",
       "      <td>[life, city, car, car, car, car, car, car, car, car]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>miss, next, penalty, little, offer, send, write, ticket, min, pick</td>\n",
       "      <td>[suppose, hella, sunny, london, next, week]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8100</td>\n",
       "      <td>pm, match, happen, start, uk, july, club, huge, evening, claim</td>\n",
       "      <td>[late, uk, time, pm]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contrib  \\\n",
       "0        0.0              0.8100   \n",
       "1        1.0              0.8643   \n",
       "2        2.0              0.8227   \n",
       "3        3.0              0.8417   \n",
       "4        4.0              0.8100   \n",
       "\n",
       "                                                                  Keywords  \\\n",
       "0        amp, great, people, work, much, child, tonight, girl, care, issue   \n",
       "1  england, game, euro, play, win, team, tomorrow, germany, football, well   \n",
       "2          pay, sure, chance, point, uk, shit, low, understand, small, car   \n",
       "3       miss, next, penalty, little, offer, send, write, ticket, min, pick   \n",
       "4           pm, match, happen, start, uk, july, club, huge, evening, claim   \n",
       "\n",
       "                                    Representative Text  \n",
       "0                 [fabulous, opportunity, great, place]  \n",
       "1      [england, beat, germany, tomorrow, england, win]  \n",
       "2  [life, city, car, car, car, car, car, car, car, car]  \n",
       "3           [suppose, hella, sunny, london, next, week]  \n",
       "4                                  [late, uk, time, pm]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "lda_20_sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "lda_20_sent_topics_outdf_grpd = lda_20_df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in lda_20_sent_topics_outdf_grpd:\n",
    "    lda_20_sent_topics_sorteddf_mallet = pd.concat([lda_20_sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "lda_20_sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "lda_20_sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "lda_20_sent_topics_sorteddf_mallet.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Representative Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7550</td>\n",
       "      <td>people, france, work, head, die, amazing, ill, spain, walk, almost</td>\n",
       "      <td>[look, look, amazing, gal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6733</td>\n",
       "      <td>death, least, cause, deserve, uk, represent, racist, binance, lord, murder</td>\n",
       "      <td>[birch, uk, ping, mum, lifer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6733</td>\n",
       "      <td>way, sure, actually, boris, people, close, family, reason, young, plan</td>\n",
       "      <td>[scotch, egg, actually, scottish]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6733</td>\n",
       "      <td>read, future, literally, possible, mp, excited, begin, saturday, opening, normal</td>\n",
       "      <td>[blood_clot, micro, chip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6733</td>\n",
       "      <td>penalty, listen, experience, together, series, instead, nowplaye, programme, dream, fast</td>\n",
       "      <td>[nick, listen, uk, drill]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contrib  \\\n",
       "0        0.0              0.7550   \n",
       "1        1.0              0.6733   \n",
       "2        2.0              0.6733   \n",
       "3        3.0              0.6733   \n",
       "4        4.0              0.6733   \n",
       "\n",
       "                                                                                   Keywords  \\\n",
       "0                        people, france, work, head, die, amazing, ill, spain, walk, almost   \n",
       "1                death, least, cause, deserve, uk, represent, racist, binance, lord, murder   \n",
       "2                    way, sure, actually, boris, people, close, family, reason, young, plan   \n",
       "3          read, future, literally, possible, mp, excited, begin, saturday, opening, normal   \n",
       "4  penalty, listen, experience, together, series, instead, nowplaye, programme, dream, fast   \n",
       "\n",
       "                 Representative Text  \n",
       "0         [look, look, amazing, gal]  \n",
       "1      [birch, uk, ping, mum, lifer]  \n",
       "2  [scotch, egg, actually, scottish]  \n",
       "3          [blood_clot, micro, chip]  \n",
       "4          [nick, listen, uk, drill]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_50_sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "lda_50_sent_topics_outdf_grpd = lda_50_df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in lda_50_sent_topics_outdf_grpd:\n",
    "    lda_50_sent_topics_sorteddf_mallet = pd.concat([lda_50_sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "lda_50_sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "lda_50_sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "lda_50_sent_topics_sorteddf_mallet.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Perplexity between LDA 20 topics and LDA 50 topics </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_20_topics = lda_20_model.log_perplexity(corpus)\n",
    "per_50_topics = lda_50_model.log_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda_20 = CoherenceModel(model=lda_20_model, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda_20 = coherence_model_lda_20.get_coherence()\n",
    "\n",
    "coherence_model_lda_50 = CoherenceModel(model=lda_50_model, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda_50 = coherence_model_lda_50.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LDA_20</th>\n",
       "      <th>LDA_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>14.815138</td>\n",
       "      <td>24.151448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coherence</th>\n",
       "      <td>0.310247</td>\n",
       "      <td>0.372020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               LDA_20     LDA_50\n",
       "Perplexity  14.815138  24.151448\n",
       "Coherence    0.310247   0.372020"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_select_df = pd.DataFrame([\n",
    "                                np.abs([per_20_topics, per_50_topics]),\n",
    "                                [coherence_lda_20, coherence_lda_50]\n",
    "                                ],\n",
    "                                columns=['LDA_20', 'LDA_50'],\n",
    "                                index=['Perplexity', 'Coherence'])\n",
    "model_select_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA with 20 topics has better perplexity score than 50 topics while coherence score 50 topics have higher value. The issues for short text topic modeling for tweets data is there are a lot of factors such as sparse words, lack of clear context, and high volume of tweets data. This factor can increase memory requirement and processing time, while reduce generalization ability of the model when chose high number of topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2. Group tweets based on some criteria. The idea here is to group similar tweets content wise and/or \n",
    "from the same users. Develop topic models on them and analyse the performance. </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all tweets: 53399\n",
      "Number of all tweeets group by username: 53399\n",
      "\n",
      " Since ID in dataset is unique which treat as different user, so LDA from Q1 can be use as group by userbaseline\n"
     ]
    }
   ],
   "source": [
    "# Group by user ID since ID in dataset is unique \n",
    "print(\"Number of all tweets:\", len(data_df))\n",
    "print(\"Number of all tweeets group by username:\", len(data_df._id.unique()))\n",
    "print(\"\\n Since ID in dataset is unique which treat as different user, so LDA from Q1 can be use as group by userbaseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal punctuation/digit/stopwords/unnesseary word before put back to sentence \n",
    "\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stoplist = set(stopwords.words('english'))\n",
    "punc = list(punctuation)\n",
    "\n",
    "def norm_words(sent):\n",
    "    #for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = re.sub(\"\\”\", \"\", sent)  # remove single quotes\n",
    "        sent = re.sub(\"\\“\", \"\", sent)  # remove single quotes\n",
    "        sent = re.sub(r'http\\S+', '', sent)\n",
    "\n",
    "        for i in punc:\n",
    "            sent = re.sub('\\\\'+i, '', sent)  \n",
    "\n",
    "        text_tokens = word_tokenize(sent.lower())\n",
    "        tokens_without_sw = [word for word in text_tokens if not word in stoplist and not word.isdigit()]\n",
    "        sent = (\" \").join(tokens_without_sw)\n",
    "        return sent\n",
    "    \n",
    "# Convert to list\n",
    "list_data = data_df.text.values.tolist()\n",
    "list_data_word = list()\n",
    "for sen in list_data:\n",
    "    list_data_word.append(norm_words(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF Vectorizer and Cosine similarity to compare similiarity of tweets\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tweet_vec = vectorizer.fit_transform(list_data_word)\n",
    "\n",
    "a = cosine_similarity(tweet_vec[0:1], tweet_vec)\n",
    "X = np.stack(( a.flatten(), np.zeros_like(a.flatten()) ), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans Cluster with 20 clusters\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    " \n",
    "kmeans = KMeans(n_clusters=20, random_state=0).fit(X)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "grouping_list = list()\n",
    "\n",
    "for i in range(20):\n",
    "    c_idx = np.where(labels==i)\n",
    "    group_tweets = \"\"\n",
    "    for j in c_idx[0]:\n",
    "        group_tweets += \" \" + list_data_word[j]\n",
    "    grouping_list.append(group_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  \n",
    "group_data_words = list(group_to_words(grouping_list))\n",
    "\n",
    "# Create Dictionary\n",
    "group_id2word = corpora.Dictionary(group_data_words)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "group_corpus = [group_id2word.doc2bow(text) for text in group_data_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model 20 topics chunksize 1000\n",
    "group_lda_20_model = gensim.models.ldamodel.LdaModel(corpus=group_corpus,\n",
    "                                           id2word=group_id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=1000,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group_lda_20_model.save('group_lda_20.model')\n",
    "#lda_20_model = gensim.models.ldamodel.LdaModel.load('lda_20.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Statistics of Group Aggregate Tweets</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aggregate Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>34027.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>126111.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>467.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3781.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10771.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>568986.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Aggregate Tweets\n",
       "count             20.00\n",
       "mean           34027.95\n",
       "std           126111.27\n",
       "min               20.00\n",
       "25%              467.75\n",
       "50%             3781.00\n",
       "75%            10771.75\n",
       "max           568986.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "group_doc_lens = [len(d) for d in group_df_dominant_topic.Text]\n",
    "group_tweets_df = pd.DataFrame(data=group_doc_lens)\n",
    "aggregate = round(group_tweets_df.describe(),2)\n",
    "aggregate.columns = ['Aggregate Tweets']\n",
    "\n",
    "display(aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate lda 20 topics model\n",
    "group_lda_20_df_topic_sents_keywords = format_topics_sentences(ldamodel=group_lda_20_model, corpus=group_corpus, texts=group_data_words)\n",
    "group_lda_20_df_dominant_topic = group_lda_20_df_topic_sents_keywords.reset_index()\n",
    "group_lda_20_df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "group_sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "group_topics_outdf_grpd = group_lda_20_df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in group_topics_outdf_grpd:\n",
    "    group_sent_topics_sorteddf_mallet = pd.concat([group_sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "group_sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "group_sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "#group_sent_topics_sorteddf_mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_per_20_topics = group_lda_20_model.log_perplexity(group_corpus)\n",
    "\n",
    "group_coherence_model_lda_20 = CoherenceModel(model=group_lda_20_model, texts=group_data_words, dictionary=group_id2word, coherence='c_v')\n",
    "group_coherence_lda_20 = group_coherence_model_lda_20.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Perplexity between LDA 20 topics and Group-LDA 20 topics </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LDA_20</th>\n",
       "      <th>GLDA_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>14.815109</td>\n",
       "      <td>9.548358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coherence</th>\n",
       "      <td>0.310247</td>\n",
       "      <td>0.262563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               LDA_20   GLDA_20\n",
       "Perplexity  14.815109  9.548358\n",
       "Coherence    0.310247  0.262563"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_model_select_df = pd.DataFrame([\n",
    "                                np.abs([per_20_topics, group_per_20_topics]),\n",
    "                                [coherence_lda_20, group_coherence_lda_20]\n",
    "                                ],\n",
    "                                columns=['LDA_20', 'GLDA_20'],\n",
    "                                index=['Perplexity', 'Coherence'])\n",
    "com_model_select_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Compare the performance differences and discuss the reasons </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LDA_20</th>\n",
       "      <th>LDA_50</th>\n",
       "      <th>G-LDA_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>14.815109</td>\n",
       "      <td>24.151444</td>\n",
       "      <td>9.548358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coherence</th>\n",
       "      <td>0.310247</td>\n",
       "      <td>0.372020</td>\n",
       "      <td>0.262563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               LDA_20     LDA_50  G-LDA_20\n",
       "Perplexity  14.815109  24.151444  9.548358\n",
       "Coherence    0.310247   0.372020  0.262563"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_performance = pd.DataFrame([\n",
    "                                np.abs([per_20_topics, per_50_topics, group_per_20_topics]),\n",
    "                                [coherence_lda_20, coherence_lda_50, group_coherence_lda_20]\n",
    "                                ],\n",
    "                                columns=['LDA_20', 'LDA_50', 'G-LDA_20'],\n",
    "                                index=['Perplexity', 'Coherence'])\n",
    "overall_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall result show LDA with 20 topics as baseline have lower perplexity than LDA with 50 topics, but when Grouping LDA with 20 topics showed decrease in perplexity value mean that it perform better than baseline. Due to grouping tweets with similar context by combine them together increase size of text which help in generalization performance of topics as tweets data tend to have similar context from tfidf and cosine similarity process. \n",
    "While coherence score in Grouping LDA with 20 topics furthure decrease as size of text become larger which penalty in finding same topic in tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic from Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Representative Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8100</td>\n",
       "      <td>amp, great, people, work, much, child, tonight, girl, care, issue</td>\n",
       "      <td>[hugh, gaitskell, die, young, well, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8643</td>\n",
       "      <td>england, game, euro, play, win, team, tomorrow, germany, football, well</td>\n",
       "      <td>[england, beat, germany, tomorrow, england, win]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8227</td>\n",
       "      <td>pay, sure, chance, point, uk, shit, low, understand, small, car</td>\n",
       "      <td>[life, city, car, car, car, car, car, car, car, car]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>miss, next, penalty, little, offer, send, write, ticket, min, pick</td>\n",
       "      <td>[suppose, hella, sunny, london, next, week]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8100</td>\n",
       "      <td>pm, match, happen, start, uk, july, club, huge, evening, claim</td>\n",
       "      <td>[late, uk, time, pm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.8100</td>\n",
       "      <td>watch, bring, guy, give, yet, oxford, remember, police, second, mate</td>\n",
       "      <td>[people, destroy, uk, mayor, london, priti, patal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.7625</td>\n",
       "      <td>report, wait, hear, woman, prime_minister, north, wish, grow, pass, eat</td>\n",
       "      <td>[flower, eat, flower, fade]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.8100</td>\n",
       "      <td>scotland, feel, fan, goal, mean, move, hour, fact, high, open</td>\n",
       "      <td>[move, apartment, september, proper, place]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.7625</td>\n",
       "      <td>break, add, happy, month, friend, complete, view, drop, currently, design</td>\n",
       "      <td>[love_island, drop, stream]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.8597</td>\n",
       "      <td>really, hope, way, conservative, actually, turn, life, people, believe, country</td>\n",
       "      <td>[hope, well, sense, morality, donation, conservative]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>france, love, thing, never, live, change, ever, world, ask, save</td>\n",
       "      <td>[really, look, france, love, news]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.8633</td>\n",
       "      <td>covid, man, case, uk, death, travel, test, rule, cause, soon</td>\n",
       "      <td>[page, admit, disappoint, wanna, peter, duncan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.8643</td>\n",
       "      <td>new, york, city, keep, far, time, back, link, early, problem</td>\n",
       "      <td>[new, york, sing, new, york, new, york]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>sign, set, lead, swiss, base, road, build, step, role, st</td>\n",
       "      <td>[practice, practice]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.8100</td>\n",
       "      <td>day, london, end, bad, school, today, vaccine, still, first, week</td>\n",
       "      <td>[uk, start, today, advanced, become]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.9441</td>\n",
       "      <td>https, co, amp, london, check, find, free, help, visit, live</td>\n",
       "      <td>[galway, cil, ce, scheme, ltd, driver, vacancy, transport, service, office, scheme, check, https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>loveisland, tell, already, house, ill, fucking, kid, price, sound, street</td>\n",
       "      <td>[holy, crap, fucking, cracker, holy, crap]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>labour, tory, year, government, vote, party, people, boris, name, player</td>\n",
       "      <td>[uk, government, tory, increase, year]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>race, least, gon, true, agree, able, usa, attack, cost, proud</td>\n",
       "      <td>[chop, plane, land, rover]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.7882</td>\n",
       "      <td>leave, talk, always, lose, long, uk, away, part, manchester, british</td>\n",
       "      <td>[gbnew, julie, churchill, defense, documents_found, bus_stop, cnnhttps, www, cnn, com, uk, defen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic_Num  Topic_Perc_Contrib  \\\n",
       "0         0.0              0.8100   \n",
       "1         1.0              0.8643   \n",
       "2         2.0              0.8227   \n",
       "3         3.0              0.8417   \n",
       "4         4.0              0.8100   \n",
       "5         5.0              0.8100   \n",
       "6         6.0              0.7625   \n",
       "7         7.0              0.8100   \n",
       "8         8.0              0.7625   \n",
       "9         9.0              0.8597   \n",
       "10       10.0              0.8417   \n",
       "11       11.0              0.8633   \n",
       "12       12.0              0.8643   \n",
       "13       13.0              0.6833   \n",
       "14       14.0              0.8100   \n",
       "15       15.0              0.9441   \n",
       "16       16.0              0.8417   \n",
       "17       17.0              0.8417   \n",
       "18       18.0              0.6833   \n",
       "19       19.0              0.7882   \n",
       "\n",
       "                                                                           Keywords  \\\n",
       "0                 amp, great, people, work, much, child, tonight, girl, care, issue   \n",
       "1           england, game, euro, play, win, team, tomorrow, germany, football, well   \n",
       "2                   pay, sure, chance, point, uk, shit, low, understand, small, car   \n",
       "3                miss, next, penalty, little, offer, send, write, ticket, min, pick   \n",
       "4                    pm, match, happen, start, uk, july, club, huge, evening, claim   \n",
       "5              watch, bring, guy, give, yet, oxford, remember, police, second, mate   \n",
       "6           report, wait, hear, woman, prime_minister, north, wish, grow, pass, eat   \n",
       "7                     scotland, feel, fan, goal, mean, move, hour, fact, high, open   \n",
       "8         break, add, happy, month, friend, complete, view, drop, currently, design   \n",
       "9   really, hope, way, conservative, actually, turn, life, people, believe, country   \n",
       "10                 france, love, thing, never, live, change, ever, world, ask, save   \n",
       "11                     covid, man, case, uk, death, travel, test, rule, cause, soon   \n",
       "12                     new, york, city, keep, far, time, back, link, early, problem   \n",
       "13                        sign, set, lead, swiss, base, road, build, step, role, st   \n",
       "14                day, london, end, bad, school, today, vaccine, still, first, week   \n",
       "15                     https, co, amp, london, check, find, free, help, visit, live   \n",
       "16        loveisland, tell, already, house, ill, fucking, kid, price, sound, street   \n",
       "17         labour, tory, year, government, vote, party, people, boris, name, player   \n",
       "18                    race, least, gon, true, agree, able, usa, attack, cost, proud   \n",
       "19             leave, talk, always, lose, long, uk, away, part, manchester, british   \n",
       "\n",
       "                                                                                    Representative Text  \n",
       "0                                                             [hugh, gaitskell, die, young, well, time]  \n",
       "1                                                      [england, beat, germany, tomorrow, england, win]  \n",
       "2                                                  [life, city, car, car, car, car, car, car, car, car]  \n",
       "3                                                           [suppose, hella, sunny, london, next, week]  \n",
       "4                                                                                  [late, uk, time, pm]  \n",
       "5                                                    [people, destroy, uk, mayor, london, priti, patal]  \n",
       "6                                                                           [flower, eat, flower, fade]  \n",
       "7                                                           [move, apartment, september, proper, place]  \n",
       "8                                                                           [love_island, drop, stream]  \n",
       "9                                                 [hope, well, sense, morality, donation, conservative]  \n",
       "10                                                                   [really, look, france, love, news]  \n",
       "11                                                      [page, admit, disappoint, wanna, peter, duncan]  \n",
       "12                                                              [new, york, sing, new, york, new, york]  \n",
       "13                                                                                 [practice, practice]  \n",
       "14                                                                 [uk, start, today, advanced, become]  \n",
       "15  [galway, cil, ce, scheme, ltd, driver, vacancy, transport, service, office, scheme, check, https...  \n",
       "16                                                           [holy, crap, fucking, cracker, holy, crap]  \n",
       "17                                                               [uk, government, tory, increase, year]  \n",
       "18                                                                           [chop, plane, land, rover]  \n",
       "19  [gbnew, julie, churchill, defense, documents_found, bus_stop, cnnhttps, www, cnn, com, uk, defen...  "
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_20_sent_topics_sorteddf_mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Representative Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>uk, england, new, like, york, london, people, get, one, would</td>\n",
       "      <td>[think, told, saj, tells, change, position, ethereum, looks, retake, days, london, hard, fork, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9748</td>\n",
       "      <td>amp, said, live, uk, new, us, people, england, london, would</td>\n",
       "      <td>[thank, campaigning, barriers, far, harm, good, amp, particularly, bad, record, london, students...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>amp, live, said, games, went, small, england, men, tour, penalty</td>\n",
       "      <td>[another, busy, week, media, duties, excellent, football, legend, clients, covering, games, amp,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.9959</td>\n",
       "      <td>amp, uk, new, london, people, us, one, get, like, england</td>\n",
       "      <td>[deadline, pm, today, apply, frontend, developer, role, youre, big, mission, flexible, working, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>arena, son, london, going, get, boy, long, tickets, due, break</td>\n",
       "      <td>[burna, boy, concert, london, arena, august, going, opener, arena, long, break, due, pandemic, g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contrib  \\\n",
       "0        4.0              1.0000   \n",
       "1        5.0              0.9748   \n",
       "2        6.0              0.9997   \n",
       "3       11.0              0.9959   \n",
       "4       17.0              0.9973   \n",
       "\n",
       "                                                           Keywords  \\\n",
       "0     uk, england, new, like, york, london, people, get, one, would   \n",
       "1      amp, said, live, uk, new, us, people, england, london, would   \n",
       "2  amp, live, said, games, went, small, england, men, tour, penalty   \n",
       "3         amp, uk, new, london, people, us, one, get, like, england   \n",
       "4    arena, son, london, going, get, boy, long, tickets, due, break   \n",
       "\n",
       "                                                                                   Representative Text  \n",
       "0  [think, told, saj, tells, change, position, ethereum, looks, retake, days, london, hard, fork, b...  \n",
       "1  [thank, campaigning, barriers, far, harm, good, amp, particularly, bad, record, london, students...  \n",
       "2  [another, busy, week, media, duties, excellent, football, legend, clients, covering, games, amp,...  \n",
       "3  [deadline, pm, today, apply, frontend, developer, role, youre, big, mission, flexible, working, ...  \n",
       "4  [burna, boy, concert, london, arena, august, going, opener, arena, long, break, due, pandemic, g...  "
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_sent_topics_sorteddf_mallet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
